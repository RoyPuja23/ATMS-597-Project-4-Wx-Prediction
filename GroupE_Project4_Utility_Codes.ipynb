{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GroupE_Project4_Utility_Codes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM05/h9Aru6GPdCwPJIw1ef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bieri2/ATMS-597-Project-4-Wx-Prediction/blob/master/GroupE_Project4_Utility_Codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSIDQmfVTr1c",
        "colab_type": "text"
      },
      "source": [
        "## Reading KCMI obs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEn5IMUAQRoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in daily KCMI data \n",
        "kcmi_daily = pd.read_csv('/content/drive/My Drive/project4_data/KCMI_daily.csv', \n",
        "                         header = 5, names = ['Timestamp', 'TMAXOBS', 'TMINOBS', \n",
        "                                              'WMAXOBS', 'RTOTOBS', 'extra'],\n",
        "                         dtype = {'TMAXOBS': np.float64, \n",
        "                                  'TMINOBS': np.float64, \n",
        "                                  'WMAXOBS': np.float64, \n",
        "                                  'RTOTOBS': np.float64}, \n",
        "                         na_values = 'M')[:-7]\n",
        "# Drop unnecessary column\n",
        "kcmi_daily = kcmi_daily.drop(columns = 'extra')\n",
        "# Set 'Date' column as index\n",
        "kcmi_daily = kcmi_daily.set_index(pd.to_datetime(kcmi_daily['Timestamp'])).drop(columns = 'Timestamp')\n",
        "# Resample to fill in missing days with NaNs\n",
        "kcmi_daily = kcmi_daily.resample('D').mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMpQTx1QQZB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in hourly KCMI data\n",
        "kcmi_hourly = pd.read_csv('/content/drive/My Drive/project4_data/KCMI_hourly.csv', \n",
        "                          header = 0, names = ['Timestamp', 'TMPC', 'DWPC', 'PRES', \n",
        "                                               'WDIR', 'WSPD', 'SKCT', 'PRCP1', \n",
        "                                               'PRCP6'], \n",
        "                          dtype = {'TMPC' : np.float64, \n",
        "                                   'DWPC' : np.float64, \n",
        "                                   'PRES' : np.float64, \n",
        "                                   'WDIR' : np.float64,\n",
        "                                   'WSPD' : np.float64, \n",
        "                                   'SKCT' : np.float64, \n",
        "                                   'PRCP1': np.float64, \n",
        "                                   'PRCP6': np.float64}, \n",
        "                          usecols = [0, 5, 6, 7, 8, 9, 10, 11, 12], na_values = 'M')\n",
        "# Set 'Timestep' column as index\n",
        "kcmi_hourly = kcmi_hourly.set_index(pd.to_datetime(kcmi_hourly['Timestamp'])).drop(columns = 'Timestamp')\n",
        "# Consider only data for 2010 to 2019\n",
        "kcmi_hourly = kcmi_hourly['2010-01-01':'2019-12-31']\n",
        "# Set trace precipitation to NaN\n",
        "kcmi_hourly['PRCP1'][kcmi_hourly['PRCP1'] < 0] = np.NaN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op0QU3k3QdpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resample hourly data to daily by summing hourly values\n",
        "kcmi_hourly_resampled = kcmi_hourly.resample('D').apply(lambda x: x.values.sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6ZMSE2UQeVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace daily precip data with aggregated hourly data, since daily precip data is problematic\n",
        "kcmi_daily['RTOTOBS'] = kcmi_hourly_resampled['PRCP1'].astype('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX-NcbPgQgGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert observations to metric units if necessary\n",
        "kcmi_daily['TMAXOBS'] = (kcmi_daily['TMAXOBS'] - 32.)/1.8 # F to C\n",
        "kcmi_daily['TMINOBS'] = (kcmi_daily['TMINOBS'] - 32.)/1.8 # F to C\n",
        "kcmi_daily['WMAXOBS'] = kcmi_daily['WMAXOBS']/2.237 # mph to m/s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAVYCn5gQhtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get data for 2019 to use for validation\n",
        "kcmi_2019  = kcmi_daily['2019-01-01':'2019-12-31']\n",
        "# Get data from 2010 to 2018 to use for training\n",
        "kcmi_train = kcmi_daily['2010-01-01':'2018-12-31']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7hPzy5VTmnc",
        "colab_type": "text"
      },
      "source": [
        "##Reading GFS data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et1PtsP2TqwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to decompress tar.gz files and add to directory\n",
        "# Only need to do this once, afterwards just need to read from Drive\n",
        "# Change paths/filenames as needed\n",
        "\n",
        "# ! mkdir '/content/drive/My Drive/project4_data/sfc_tar'\n",
        "# ! gunzip '/content/drive/My Drive/project4_data/sfc.tar.gz' \n",
        "# ! tar -xvf '/content/drive/My Drive/project4_data/sfc.tar' --directory '/content/drive/My Drive/project4_data/sfc_tar'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7jASIwiTxcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Paths to files for different GFS datasets\n",
        "# Change as needed\n",
        "gfs_daily_dir = '/content/drive/My Drive/project4_data/daily_tar/bufkit/' # Daily GFS data\n",
        "gfs_prof_dir  = '/content/drive/My Drive/project4_data/prof_tar/bufkit/'  # GFS 3-hr vertical profile (not currently used)\n",
        "gfs_sfc_dir   = '/content/drive/My Drive/project4_data/sfc_tar/bufkit/'   # GFS 3-hr surface data (not currently used)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFdT8xZVT1pC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define function to read in GFS daily data one day at a time\n",
        "# Once all files have been read, combine into one DataFrame \n",
        "def get_gfs_data(gfs_dir, prof = False):\n",
        "\n",
        "  # Get list of files to be read\n",
        "  files = os.listdir(gfs_dir)\n",
        "  # Sort so that files are in order by date\n",
        "  files.sort()\n",
        "  # Create list of full paths for all files\n",
        "  file_list = [gfs_dir + f for f in files]\n",
        "\n",
        "  # Create empty list to hold data\n",
        "  if prof:\n",
        "      all_dwpc = []\n",
        "      all_hght = []\n",
        "      all_tmpc = []\n",
        "      all_uwnd = []\n",
        "      all_vwnd = [] \n",
        "  else:\n",
        "      all_dfs  = []\n",
        "\n",
        "\n",
        "  # Read in each file and add to list\n",
        "  for f in file_list: \n",
        "      # Tell user which file is being read\n",
        "      print('Reading ' + f)\n",
        "\n",
        "      if prof:\n",
        "          current = pd.read_csv(f, index_col = 0, header = 0,\n",
        "                                names = ['Timestep', 'DWPC', 'HGHT', \n",
        "                                         'PRES', 'TMPC', 'UWND', 'VWND'])  \n",
        "         # print(current['PRES'])\n",
        "\n",
        "          for i in current.index:\n",
        "              dwpc_current = pd.DataFrame(current['DWPC'][i][1:-1].split(',')).T\n",
        "              dwpc_current['Timestep'] = [dt.datetime.strptime(i, '%Y-%m-%d %H:%M:%S')]\n",
        "              dwpc_current = dwpc_current.set_index('Timestep')\n",
        "              dwpc_current = dwpc_current.rename(columns = {0:'925', 1:'850', 2:'700', \n",
        "                                                            3:'500', 4:'250', 5:'100'})\n",
        "              all_dwpc.append(dwpc_current) \n",
        "\n",
        "              hght_current = pd.DataFrame(current['HGHT'][i][1:-1].split(',')).T \n",
        "              hght_current['Timestep'] = [dt.datetime.strptime(i, '%Y-%m-%d %H:%M:%S')]\n",
        "              hght_current = hght_current.set_index('Timestep')\n",
        "              hght_current = hght_current.rename(columns = {0:'925', 1:'850', 2:'700', \n",
        "                                                            3:'500', 4:'250', 5:'100'})\n",
        "              all_hght.append(hght_current) \n",
        "\n",
        "              tmpc_current = pd.DataFrame(current['TMPC'][i][1:-1].split(',')).T \n",
        "              tmpc_current['Timestep'] = [dt.datetime.strptime(i, '%Y-%m-%d %H:%M:%S')]\n",
        "              tmpc_current = tmpc_current.set_index('Timestep')\n",
        "              tmpc_current = tmpc_current.rename(columns = {0:'925', 1:'850', 2:'700', \n",
        "                                                            3:'500', 4:'250', 5:'100'})\n",
        "              all_tmpc.append(tmpc_current) \n",
        "\n",
        "              uwnd_current = pd.DataFrame(current['UWND'][i][1:-1].split(',')).T \n",
        "              uwnd_current['Timestep'] = [dt.datetime.strptime(i, '%Y-%m-%d %H:%M:%S')]\n",
        "              uwnd_current = uwnd_current.set_index('Timestep')\n",
        "              uwnd_current = uwnd_current.rename(columns = {0:'925', 1:'850', 2:'700', \n",
        "                                                            3:'500', 4:'250', 5:'100'})\n",
        "              all_uwnd.append(uwnd_current) \n",
        "\n",
        "              vwnd_current = pd.DataFrame(current['VWND'][i][1:-1].split(',')).T \n",
        "              vwnd_current['Timestep'] = [dt.datetime.strptime(i, '%Y-%m-%d %H:%M:%S')]\n",
        "              vwnd_current = vwnd_current.set_index('Timestep')\n",
        "              vwnd_current = vwnd_current.rename(columns = {0:'925', 1:'850', 2:'700', \n",
        "                                                            3:'500', 4:'250', 5:'100'})\n",
        "              all_vwnd.append(vwnd_current) \n",
        "\n",
        "      else:\n",
        "          current = pd.read_csv(f)\n",
        "          current = current.rename(columns={'Unnamed: 0': 'Timestamp', \n",
        "                                      'TMAX': 'TMAXGFS',\n",
        "                                      'TMIN': 'TMINGFS',\n",
        "                                      'WMAX': 'WMAXGFS', \n",
        "                                      'RTOT': 'RTOTGFS'})\n",
        "          # Set timestamp as index\n",
        "          current = current.set_index(pd.to_datetime(current['Timestamp'])).drop(columns = 'Timestamp')\n",
        "\n",
        "          all_dfs.append(current) \n",
        "\n",
        "  if prof:\n",
        "      all_dwpc = pd.concat(all_dwpc).astype(np.float64)\n",
        "      all_hght = pd.concat(all_hght).astype(np.float64)\n",
        "      all_tmpc = pd.concat(all_tmpc).astype(np.float64)\n",
        "      all_uwnd = pd.concat(all_uwnd).astype(np.float64)\n",
        "      all_vwnd = pd.concat(all_vwnd).astype(np.float64)\n",
        "\n",
        "      return [all_dwpc, all_hght, all_tmpc, all_uwnd, all_vwnd]\n",
        "\n",
        "  else: \n",
        "      # Create one DataFrame with all data \n",
        "      all_dfs = pd.concat(all_dfs).astype(np.float64)\n",
        "\n",
        "      return all_dfs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91robPiLT6yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in profile GFS data\n",
        "dwpc, hght, tmpc, uwnd, vwnd = get_gfs_data(gfs_prof_dir, prof = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9rcm1PoT7kH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in daily GFS data\n",
        "# This will take several minutes the first time but will be a lot faster if run again without restarting runtime\n",
        "gfs_daily = get_gfs_data(gfs_daily_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adiI35s9UAdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resample to fill any missing days\n",
        "gfs_daily = gfs_daily.resample('D').mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHF7VxisUCwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Increase all dates in GFS data indices by one day since GFS forecasts apply to the following day\n",
        "gfs_daily = gfs_daily.set_index(gfs_daily.index + pd.Timedelta(days = 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S968nE_UES_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separate into training and validation datasets\n",
        "gfs_2019  = gfs_daily['2019-01-01':'2019-12-31']\n",
        "gfs_train = gfs_daily['2010-01-02':'2018-12-31']"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}